# ğŸ§  RAG-Based ChatBot â€“ Second Phase (Flask UI + Local LLaMA)

This is the **second-phase branch** of my RAG (Retrieval-Augmented Generation) ChatBot project. In this phase, I moved from a terminal-only prototype to a full-stack web app using **Flask**, while still keeping everything local using Metaâ€™s LLaMA 2 7B Chat model.

---

## âœ… What Iâ€™ve Done So Far

### ğŸ§  Local Model Inference

* Using **Meta's LLaMA 2 7B Chat (GGUF)** model locally
* Loaded the quantized `.gguf` model with `llama-cpp-python`
* Created a reusable function to generate answers from prompts

### ğŸŒ Web Search with Serper.dev

* Integrated **Serper.dev** to fetch real-time Google search results
* Modularized logic inside `serper_search.py`
* Merged results with prompt templates to guide LLaMA's generation

### ğŸ’» Flask Backend

* Created a RESTful API using **Flask**
* `/ask` endpoint accepts POST requests with a question
* Response is generated using LLaMA + Serper search
* Kept debug off and multi-threading enabled for smooth local testing

### ğŸ’¬ Basic Web UI (Phase 1)

* Built a simple but neat web UI using HTML, CSS, and JS
* Chat interface mimics real conversation
* Removed background animations to save compute
* Optimized user input behavior and auto-scroll chat history

### ğŸ§¹ Project Clean-up

* `.gitignore` filters out `.env`, `venv/`, and `.gguf` model files
* `requirements.txt` updated to include Flask and other packages
* Folder structure organized for clarity

---

## ğŸ“ Current Project Structure

```bash
â”œâ”€â”€ app.py                 # Flask backend
â”œâ”€â”€ rag_chat.py            # RAG logic (search + prompt + LLM)
â”œâ”€â”€ llama_local.py         # Local LLaMA model inference
â”œâ”€â”€ serper_search.py       # Web search API logic
â”œâ”€â”€ templates/
â”‚   â””â”€â”€ index.html         # Web chat layout
â”œâ”€â”€ static/
â”‚   â””â”€â”€ style.css          # Chat UI design
â”œâ”€â”€ .env                   # Stores SERPER_API_KEY (excluded from Git)
â”œâ”€â”€ .gitignore             # Excludes venv, env vars, model files
â”œâ”€â”€ requirements.txt       # All Python dependencies
```

---

## ğŸš§ Future Updates

| Feature                             | Status     |
| ----------------------------------- | ---------- |
| ğŸŒ Replace local LLaMA with LLM API | ğŸ”œ Planned |
| ğŸš€ Deploy on Render or Railway      | ğŸ”œ Planned |
| ğŸ’¾ Add conversation logging         | ğŸ”œ Later   |
| âœ¨ Enhance UI with dynamic effects   | ğŸ”œ Later   |

---

## ğŸ‘¤ About Me

Built by **Khadhar Hameed Khan Pathan**, a master's student passionate about applied NLP and full-stack AI products.

> "From scratch to smart â€” building my own ChatGPT with RAG + LLaMA from the ground up."

---

ğŸ” Stay tuned for API integration and full cloud deployment in the next phase!
