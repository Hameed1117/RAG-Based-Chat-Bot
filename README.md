# ğŸ§  RAG-Based ChatBot (Local LLaMA 2 + Web Search)

This project implements a **Retrieval-Augmented Generation (RAG)** chatbot powered by:

- ğŸ” **Real-time web search** using [Serper.dev (Google Search API)](https://serper.dev)
- ğŸ§  **Local inference** using **Metaâ€™s LLaMA 2 7B Chat** model (GGUF format)
- âœ… Fully **offline generation**, no token limits, no OpenAI dependence
- ğŸ“¦ Packaged in a clean Python setup with a virtual environment

---

## ğŸ“ Project Structure

| File/Folder              | Purpose |
|--------------------------|---------|
| `llama_local.py`         | Loads the local LLaMA 2 GGUF model and defines the inference function |
| `test_local_llama.py`    | Test script to verify LLaMA 2 responds correctly to a static prompt |
| `web_search.py`          | Connects to Serper.dev to fetch real-time search results |
| `rag_chat.py`            | Main chatbot logic: searches â†’ builds prompt â†’ gets LLaMA reply |
| `models/`                | Stores the downloaded GGUF model (`llama-2-7b-chat.Q4_K_M.gguf`) |
| `venv/`                  | Python 3.10.10 virtual environment |
| `.env` (optional)        | Store `SERPER_API_KEY` here if desired (currently hardcoded) |

---

## âœ… What We've Done So Far

1. ğŸ”§ Created Python 3.10.10 virtual environment
2. ğŸ’¡ Chose **LLaMA 2 7B Chat** as our base model for generation
3. ğŸ”½ Downloaded quantized `GGUF` model from Hugging Face (TheBloke)
4. ğŸš€ Loaded it using `llama-cpp-python` for fast local inference
5. ğŸŒ Integrated Serper API to fetch Google search snippets
6. ğŸ”„ Built a full **RAG loop** combining retrieval + generation
7. ğŸ§ª Successfully tested real-world queries (e.g., current date, events, ML definitions)

---

## âš™ï¸ Setup Instructions

### 1. Create and Activate Virtual Environment

```bash
python -m venv venv
venv\Scripts\activate  # Windows
````

### 2. Install Dependencies

```bash
pip install llama-cpp-python requests
```

### 3. Download LLaMA 2 Model (GGUF)

Use real curl:

```bash
curl -L -o models/llama-2-7b-chat.Q4_K_M.gguf https://huggingface.co/TheBloke/Llama-2-7B-Chat-GGUF/resolve/main/llama-2-7b-chat.Q4_K_M.gguf
```

### 4. Add Your Serper API Key

Edit `web_search.py` and set your key:

```python
SERPER_API_KEY = "your_real_key"
```

---

## ğŸ§ª Run the Bot

```bash
python rag_chat.py
```

Type your question. Example:

```
what is today's date and what events are happening this week?
```

---

## ğŸš§ Future Improvements

| Feature                                 | Status      |
| --------------------------------------- | ----------- |
| ğŸ” Terminal chat loop                   | âœ… Done      |
| ğŸŒ Web search (Serper) integration      | âœ… Done      |
| ğŸ§  Local model inference                | âœ… Done      |
| ğŸ—‚ï¸ Convert to modular Flask API        | ğŸ”œ Next     |
| ğŸ–¼ï¸ Add custom web UI                   | ğŸ”œ Planned  |
| ğŸŒ Deploy to GitHub + personal VPS      | ğŸ”œ Planned  |
| ğŸ§ª Add RAG benchmarking + eval          | ğŸ”œ Optional |
| âœï¸ Prompt refinement / context chunking | ğŸ”œ Future   |

---

## ğŸ“Œ License & Credits

* Model: [Meta LLaMA 2 7B Chat](https://ai.meta.com/llama/)
* GGUF: [TheBloke on Hugging Face](https://huggingface.co/TheBloke)
* Web Search: [Serper.dev](https://serper.dev)
* Engine: [llama-cpp-python](https://github.com/abetlen/llama-cpp-python)

---

Built with â¤ï¸ by a passionate AI engineer in the making.

```

---

Would you like me to package this in a file or also generate a `requirements.txt` and `.gitignore` to go with it?
```
