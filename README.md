# ğŸ§  RAG-Based ChatBot (Local LLaMA 2 + Real-Time Web Search)

This project implements a **Retrieval-Augmented Generation (RAG)** chatbot using a local LLM and real-time web search integration â€” enabling factual, contextual, and up-to-date answers with zero dependency on OpenAI or remote inference.

## ğŸš€ What's Inside

| Component        | Description                                                              |
|------------------|--------------------------------------------------------------------------|
| ğŸ§  **LLaMA 2 7B Chat**   | Runs locally via `llama-cpp-python` using a quantized GGUF model      |
| ğŸ” **Web Search**       | Integrated with `Serper.dev` (Google Search API) for real-time data   |
| ğŸ§  **RAG Loop**         | Combines retrieved snippets with prompt â†’ sends to LLaMA               |
| ğŸ§ª **Test Scripts**     | For validating local generation, search APIs, and full chat pipeline   |
| ğŸ’» **Terminal Interface** | Interactive chat prompt with exit option for manual testing             |

---

## ğŸ—‚ï¸ Project Structure

```text
ğŸ“ NLP-Rag Based Chat BOT/
â”œâ”€â”€ llama_local.py         # Loads LLaMA 2 GGUF and defines inference
â”œâ”€â”€ new_web_search.py      # Returns RAG-friendly formatted search results
â”œâ”€â”€ serper_search.py       # Returns raw snippet list for testing/analysis
â”œâ”€â”€ rag_chat.py            # Main chatbot: retrieval â†’ generation
â”œâ”€â”€ test_local_llama.py    # Test local LLaMA inference
â”œâ”€â”€ test_serper.py         # Test raw Serper API response
â”œâ”€â”€ test_rag.py            # Full RAG test (search + generate)
â”œâ”€â”€ models/
â”‚   â””â”€â”€ llama-2-7b-chat.Q4_K_M.gguf
â”œâ”€â”€ .env                   # Stores SERPER_API_KEY
â”œâ”€â”€ .gitignore
â”œâ”€â”€ README.md
â”œâ”€â”€ requirements.txt
â””â”€â”€ venv/                  # Python 3.10.10 virtual environment
````


## âš™ï¸ Setup Instructions

### 1. Create and Activate Environment

```bash
python -m venv venv
venv\Scripts\activate  # On Windows
```

### 2. Install Dependencies

```bash
pip install -r requirements.txt
```

### 3. Download LLaMA 2 GGUF Model

```bash
curl -L -o models/llama-2-7b-chat.Q4_K_M.gguf https://huggingface.co/TheBloke/Llama-2-7B-Chat-GGUF/resolve/main/llama-2-7b-chat.Q4_K_M.gguf
```

### 4. Set Your Serper API Key

Create a `.env` file:

```
SERPER_API_KEY=your_key_here
```

---

## ğŸ§ª Run the Bot (Terminal Interface)

```bash
python rag_chat.py
```

Sample:

```
ğŸ§‘ You: Whatâ€™s the latest on AI safety?
ğŸ§  Bot: [Answer generated using real-time search + LLaMA]
```

---

## âœ… Completed Milestones

* ğŸ” Terminal chat loop
* ğŸŒ Web search integration (Serper)
* ğŸ§  Local LLaMA 2 model inference
* ğŸ“ Modular project structure with test cases
* ğŸ“¦ Ready for Phase 2: Flask API

---

## ğŸ”œ Next Steps (Coming in `api-flask` branch)

| Feature                    | Status     |
| -------------------------- | ---------- |
| ğŸ—‚ï¸ Flask API endpoints    | ğŸ”œ Planned |
| ğŸ–¼ï¸ Custom Web UI frontend | ğŸ”œ Planned |
| ğŸŒ VPS / GitHub deployment | ğŸ”œ Planned |

---

## ğŸ“Œ Credits

* **Model**: Meta LLaMA 2 7B Chat
* **Quantization**: TheBloke (GGUF format)
* **Search API**: Serper.dev
* **Engine**: llama-cpp-python


Built with â¤ï¸ by Khadhar Hameed Khan Pathan â€“ a passionate AI engineer in the making.

````
